{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.spatial import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getdatafromfile():\n",
    "    classes=['athletics','cricket','football','rugby','tennis']\n",
    "    list=[glob.glob('bbcsport/{}/*'.format(classes[i])) for i in range(len(classes))]\n",
    "    return list,classes\n",
    "\n",
    "def filesInEachClass(list):\n",
    "    arr=[len(i) for i in list]\n",
    "    print(\"count of total files in each class\",arr)\n",
    "    return arr\n",
    "\n",
    "def trainCount(list, x=0.30):\n",
    "    train_count=[len(i)-(int(len(i)*(x))) for i in list]\n",
    "    print(\"count of train data in each class\",train_count)\n",
    "    return train_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train word index\n",
    "\n",
    "def createIndex(Train_documents):\n",
    "    word_index={}\n",
    "    for doc in range(len(Train_documents)):\n",
    "        for word in Train_documents[doc]:\n",
    "            if word not in word_index:\n",
    "                word_index[word]={}\n",
    " \n",
    "    return word_index\n",
    "\n",
    "\n",
    "def preprocessData(list,classes,train_count):\n",
    "\n",
    "#     Train_documents=[]\n",
    "    \n",
    "#     Test_documents=[]\n",
    "#     Train_set=[]\n",
    "    \n",
    "    alldocDict={}\n",
    "\n",
    "    for mainClass in range(len(list)): \n",
    "\n",
    "#         count=0\n",
    "        for subFiles in range(len(list[mainClass])):\n",
    "#             count+=1\n",
    "    #         print(len(list[mainClass]))\n",
    "    #         print(list[mainClass][subFiles])\n",
    "\n",
    "            f=open(list[mainClass][subFiles],'r')\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "            # convert to lower case\n",
    "            fullfile = tokenizer.tokenize(f.read().lower())\n",
    "\n",
    "            # stem document\n",
    "            stemmedDocs=[porter_stemmer.stem(word) for word in fullfile]\n",
    "\n",
    "            #trimming the file name and removing redundant '.txt'\n",
    "            p=os.path.basename(list[mainClass][subFiles])\n",
    "            p=p.split('.')[0]\n",
    "\n",
    "            # remove all tokens that are not alphabetic and stop words\n",
    "            tokens_without_sw = [word for word in stemmedDocs if word not in stop_words and word.isalpha()]\n",
    "\n",
    "            alldocDict[mainClass,int(p)]=tokens_without_sw\n",
    "\n",
    "            # append into train and test set\n",
    "#             if(count <= train_count[mainClass]):\n",
    "#                 Train_documents.append((mainClass,int(p))) #\n",
    "#                 Train_set.append(tokens_without_sw)\n",
    "                \n",
    "# #                 Train_label.append(mainClass)\n",
    "#             else:\n",
    "#                 Test_documents.append((mainClass,int(p)))\n",
    "#                 Test_label.append(mainClass)\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"train documents\",len(Train_documents))\n",
    "#     print(\"test documents\",len(Test_documents))\n",
    "#     print(\"total\", len(Train_documents)+len(Test_documents))\n",
    "    \n",
    "#     return Train_documents, Train_set ,Test_documents ,alldocDict\n",
    "    return alldocDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainTestList(totalfiles,alldocDict,train_count,classes):\n",
    "    Train_documents=[]\n",
    "    Test_documents=[]\n",
    "    Train_set=[]\n",
    "    Train_Init=[]\n",
    "    for classId in range(len(classes)):\n",
    "        Train_Init.append(random.sample(range(1, totalfiles[classId]), train_count[classId]))\n",
    "#         print(classId,len(Train_Init[classId]),Train_Init[classId])\n",
    "\n",
    "    for classId in range(len(classes)):\n",
    "        print(classId,\"  -----> \")\n",
    "        for i in Train_Init[classId]:\n",
    "            Train_documents.append((classId,i))\n",
    "            Train_set.append(alldocDict[(classId,i)])\n",
    "        print(len(Train_documents),\"-------------------------------------------------------\")\n",
    "        \n",
    "    for class_id in alldocDict.keys():\n",
    "        if(class_id not in Train_documents):\n",
    "            Test_documents.append(class_id)\n",
    "#             print(class_id)\n",
    "            \n",
    "#     print(len(Train_set))\n",
    "    print(len(Test_documents))\n",
    "    print(len(Train_documents))\n",
    "\n",
    "    return Train_documents, Train_set, Test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of train data in each class [71, 87, 186, 103, 70]\n",
      "count of total files in each class [101, 124, 265, 147, 100]\n"
     ]
    }
   ],
   "source": [
    "list,classes=getdatafromfile()\n",
    "# noOfdata=filesInEachClass(list)\n",
    "train_count=trainCount(list,0.30)\n",
    "totalFiles=filesInEachClass(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -----> \n",
      "71 -------------------------------------------------------\n",
      "1   -----> \n",
      "158 -------------------------------------------------------\n",
      "2   -----> \n",
      "344 -------------------------------------------------------\n",
      "3   -----> \n",
      "447 -------------------------------------------------------\n",
      "4   -----> \n",
      "517 -------------------------------------------------------\n",
      "220\n",
      "517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train_documents, Train_set, Test_documents, \n",
    "\n",
    "alldocDict=preprocessData(list,classes,train_count)\n",
    "Train_documents, Train_set, Test_documents=GetTrainTestList(totalFiles,alldocDict,train_count,classes)\n",
    "# print(\"train Ids:\\n\",Train_documents)\n",
    "# print(\"test Ids:\\n\", Test_documents)\n",
    "\n",
    "# print(alldocDict[0,1])       # [0-->classID,1-->docID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tfidf score\n",
    "\n",
    "def tfidfCalculation(term_index, alldocDict,Train_documents):\n",
    "    f = open(\"TFIDFdict_allDocs.txt\",\"w\")\n",
    "    \n",
    "    idf={}\n",
    "    for word in term_index.keys():\n",
    "        df=0\n",
    "        for doc in alldocDict.keys():\n",
    "            if word in alldocDict[doc]:\n",
    "                df+=1\n",
    "        idf[word]=math.log(len(Train_documents)/(df))\n",
    "        \n",
    "\n",
    "    for word in term_index.keys():\n",
    "        for doc in alldocDict.keys():      \n",
    "            if word in alldocDict[doc]:\n",
    "    #             print('word found in ',doc)   \n",
    "                term_index[word][doc]=((alldocDict[doc].count(word))*idf[word])  #calculate tfidf\n",
    "            else:\n",
    "                term_index[word][doc]=0\n",
    "                \n",
    "    f.write(str(term_index))\n",
    "    f.close()\n",
    "                \n",
    "    return term_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_documents, Test_documents, tfidf_dict\n",
    "\n",
    "def formVectors(Train_documents, Test_documents, tfidf_dict):\n",
    "\n",
    "    #form train and test vectors\n",
    "    trainDoc_vect={}\n",
    "    testDoc_vect={}\n",
    "    # for word in tfidf_dict.keys():\n",
    "\n",
    "    f1 = open(\"TrainDocs_vector.txt\",\"w\")\n",
    "    f2 = open(\"TestDocs_allDocs.txt\",\"w\")\n",
    "\n",
    "    for docid in Train_documents:\n",
    "        trainDoc_vect[docid]=[]\n",
    "        for word in tfidf_dict.keys():\n",
    "            trainDoc_vect[docid].append(tfidf_dict[word][docid])\n",
    "\n",
    "    for docid in Test_documents:\n",
    "        testDoc_vect[docid]=[]\n",
    "        for word in tfidf_dict.keys():\n",
    "            testDoc_vect[docid].append(tfidf_dict[word][docid])\n",
    "\n",
    "    f1.write(str(trainDoc_vect))\n",
    "    f1.close()\n",
    "    f2.write(str(testDoc_vect))\n",
    "    f2.close()\n",
    "    \n",
    "    return trainDoc_vect,testDoc_vect\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=createIndex(Train_set) # index of words in training set(train_set has all the words that are in training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict=tfidfCalculation(word_index, alldocDict,Train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVector,testVector=formVectors(Train_documents, Test_documents, tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(testVector)+len(trainVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsimilarity={}\n",
    "for testid in testVector:\n",
    "    testsimilarity[testid]={}\n",
    "    for trainid in trainVector:\n",
    "        testsimilarity[testid][trainid]=(distance.cosine(testVector[testid], trainVector[trainid]))\n",
    "#         distance.euclidean(testVector[testid], trainVector[trainid])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 0\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 2\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "no=0\n",
    "for i in testsimilarity:\n",
    "    n1,n2,n3=sorted(testsimilarity[i], key=testsimilarity[i].get)[0:3]\n",
    "    newList=[n1[0],n2[0],n3[0]]\n",
    "    frequentNN=max(set(newList), key = newList.count)\n",
    "    print(i[0],frequentNN)\n",
    "    if(i[0]==frequentNN):\n",
    "        correct+=1\n",
    "    else:\n",
    "        no+=1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "98.18181818181819\n"
     ]
    }
   ],
   "source": [
    "print(no)\n",
    "print((correct/(no+correct))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
