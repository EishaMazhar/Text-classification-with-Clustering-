{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.spatial import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "import random\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdatafromfile():\n",
    "    classes=['athletics','cricket','football','rugby','tennis']\n",
    "    list=[glob.glob('bbcsport/{}/*'.format(classes[i])) for i in range(len(classes))]\n",
    "    return list,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationKNN:\n",
    "    def __init__(self,list,classes):\n",
    "        self.list=list\n",
    "        self.classes=classes\n",
    "        self.totalfiles=[len(i) for i in list]\n",
    "        print(\"Total files in each class: \",self.totalfiles)\n",
    "        \n",
    "        \n",
    "    def preprocessData(self):\n",
    "\n",
    "        self.alldocDict={}\n",
    "        for mainClass in range(len(self.list)): \n",
    "            for subFiles in range(len(self.list[mainClass])):\n",
    "                f=open(self.list[mainClass][subFiles],'r')\n",
    "                tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                # convert to lower case\n",
    "                fullfile = tokenizer.tokenize(f.read().lower())\n",
    "                # stem document\n",
    "                stemmedDocs=[porter_stemmer.stem(word) for word in fullfile]\n",
    "                #trimming the file name and removing redundant '.txt'\n",
    "                p=os.path.basename(self.list[mainClass][subFiles])\n",
    "                p=p.split('.')[0]\n",
    "                # remove all tokens that are not alphabetic and stop words\n",
    "                tokens_without_sw = [word for word in stemmedDocs if word not in stop_words and word.isalpha()]\n",
    "                self.alldocDict[mainClass,int(p)]=tokens_without_sw\n",
    "\n",
    "        return self.alldocDict\n",
    "    \n",
    "    \n",
    "    def SplitTrainTestDocs(self,testRatio):    \n",
    "        \n",
    "        self.train_count=[len(i)-(int(len(i)*(testRatio))) for i in self.list]\n",
    "        \n",
    "        self.Train_documents=[]\n",
    "        self.Test_documents=[]\n",
    "        self.Train_set=[]\n",
    "        \n",
    "        Train_Init=[]\n",
    "        for classId in range(len(self.classes)):\n",
    "            Train_Init.append(random.sample(range(1, self.totalfiles[classId]), self.train_count[classId]))\n",
    "\n",
    "        for classId in range(len(self.classes)):\n",
    "            for i in Train_Init[classId]:\n",
    "                self.Train_documents.append((classId,i))\n",
    "                self.Train_set.append(self.alldocDict[(classId,i)])\n",
    "\n",
    "        for class_id in self.alldocDict.keys():\n",
    "            if(class_id not in self.Train_documents):\n",
    "                self.Test_documents.append(class_id)\n",
    "\n",
    "        print(\"Number of test documents: \",len(self.Test_documents))\n",
    "        print(\"Number of train documents: \",len(self.Train_documents))\n",
    "        \n",
    "        # index of words in training set(train_set has all the words that are in training set)\n",
    "        self.word_index={}\n",
    "        for doc in range(len(self.Train_set)):\n",
    "            for word in self.Train_set[doc]:\n",
    "                if word not in self.word_index:\n",
    "                    self.word_index[word]={}\n",
    "                    \n",
    "        print(len(self.word_index))\n",
    "\n",
    "        return self.Train_documents, self.Test_documents, self.word_index\n",
    "    \n",
    "    \n",
    "    def tfidfCalculation(self,term_index):\n",
    "        \n",
    "        f = open(\"TFIDFdict_allDocs.txt\",\"w\")\n",
    "        idf={}\n",
    "        for word in term_index.keys():\n",
    "            df=0\n",
    "            for doc in self.alldocDict.keys():\n",
    "                if word in self.alldocDict[doc]:\n",
    "                    df+=1\n",
    "            idf[word]=math.log(len(self.Train_documents)/(df))\n",
    "\n",
    "        for word in term_index.keys():\n",
    "            for doc in self.alldocDict.keys():      \n",
    "                if word in self.alldocDict[doc]:\n",
    "        #             print('word found in ',doc)   \n",
    "                    term_index[word][doc]=((self.alldocDict[doc].count(word))*idf[word])  #calculate tfidf\n",
    "                else:\n",
    "                    term_index[word][doc]=0\n",
    "        f.write(str(term_index))\n",
    "        f.close()\n",
    "        \n",
    "        return term_index\n",
    "    \n",
    "    def formVectors(self,tfidf_dict):\n",
    "\n",
    "        #form train and test vectors\n",
    "        self.trainDoc_vect={}\n",
    "        self.testDoc_vect={}\n",
    "\n",
    "        f1 = open(\"TrainDocs_vector.txt\",\"w\")\n",
    "        f2 = open(\"TestDocs_allDocs.txt\",\"w\")\n",
    "\n",
    "        for docid in self.Train_documents:\n",
    "            self.trainDoc_vect[docid]=[]\n",
    "            for word in tfidf_dict.keys():\n",
    "                self.trainDoc_vect[docid].append(tfidf_dict[word][docid])\n",
    "\n",
    "        for docid in self.Test_documents:\n",
    "            self.testDoc_vect[docid]=[]\n",
    "            for word in tfidf_dict.keys():\n",
    "                self.testDoc_vect[docid].append(tfidf_dict[word][docid])\n",
    "\n",
    "        f1.write(str(self.trainDoc_vect))\n",
    "        f1.close()\n",
    "        f2.write(str(self.testDoc_vect))\n",
    "        f2.close()\n",
    "\n",
    "        return self.trainDoc_vect, self.testDoc_vect\n",
    "    \n",
    "    \n",
    "    def calculateSimilarityAndKNN(self):\n",
    "        self.testsimilarity={}\n",
    "        for testid in self.testDoc_vect:\n",
    "            self.testsimilarity[testid]={}\n",
    "            for trainid in self.trainDoc_vect:\n",
    "                self.testsimilarity[testid][trainid]=(distance.cosine(self.testDoc_vect[testid], self.trainDoc_vect[trainid]))\n",
    "        self.correct=0\n",
    "        self.no=0\n",
    "        for i in self.testsimilarity:\n",
    "            n1,n2,n3=sorted(self.testsimilarity[i], key=self.testsimilarity[i].get)[0:3]\n",
    "            newList=[n1[0],n2[0],n3[0]]\n",
    "            frequentNN=max(set(newList), key = newList.count)\n",
    "        #     print(i[0],frequentNN)\n",
    "            if(i[0]==frequentNN):\n",
    "                self.correct+=1\n",
    "            else:\n",
    "                self.no+=1\n",
    "    \n",
    "    \n",
    "    def getAccuracy(self):\n",
    "        return (self.correct/(self.no+self.correct))*100\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in each class:  [101, 124, 265, 147, 100]\n",
      "Number of test documents:  220\n",
      "Number of train documents:  517\n",
      "7956\n"
     ]
    }
   ],
   "source": [
    "list,classes=getdatafromfile()\n",
    "clf=TextClassificationKNN(list,classes)\n",
    "alldocDict=clf.preprocessData()\n",
    "Train_documents, Test_documents, word_index=clf.SplitTrainTestDocs(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict=clf.tfidfCalculation(word_index)\n",
    "trainVector,testVector=clf.formVectors(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.calculateSimilarityAndKNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0909090909091\n"
     ]
    }
   ],
   "source": [
    "#due to random sampling, accuracy will vary between 96-99%\n",
    "\n",
    "print(clf.getAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
