{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.spatial import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getdatafromfile():\n",
    "    classes=['athletics','cricket','football','rugby','tennis']\n",
    "    list=[glob.glob('bbcsport/{}/*'.format(classes[i])) for i in range(len(classes))]\n",
    "    return list,classes\n",
    "\n",
    "def filesInEachClass(list):\n",
    "    arr=[len(i) for i in list]\n",
    "    print(\"count of total files in each class\",arr)\n",
    "    return arr\n",
    "\n",
    "def trainCount(list, x=0.30):\n",
    "    train_count=[len(i)-(int(len(i)*(x))) for i in list]\n",
    "    print(\"count of train data in each class\",train_count)\n",
    "    return train_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(list,classes,train_count):\n",
    "\n",
    "    Train_documents=[]\n",
    "    \n",
    "    Test_documents=[]\n",
    "    Train_set=[]\n",
    "    \n",
    "    alldocDict={}\n",
    "\n",
    "    for mainClass in range(len(list)): \n",
    "\n",
    "        count=0\n",
    "        for subFiles in range(len(list[mainClass])):\n",
    "            count+=1\n",
    "    #         print(len(list[mainClass]))\n",
    "    #         print(list[mainClass][subFiles])\n",
    "\n",
    "            f=open(list[mainClass][subFiles],'r')\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "            # convert to lower case\n",
    "            fullfile = tokenizer.tokenize(f.read().lower())\n",
    "\n",
    "            # stem document\n",
    "            stemmedDocs=[porter_stemmer.stem(word) for word in fullfile]\n",
    "\n",
    "            #trimming the file name and removing redundant '.txt'\n",
    "            p=os.path.basename(list[mainClass][subFiles])\n",
    "            p=p.split('.')[0]\n",
    "\n",
    "            # remove all tokens that are not alphabetic and stop words\n",
    "            tokens_without_sw = [word for word in stemmedDocs if word not in stop_words and word.isalpha()]\n",
    "\n",
    "            alldocDict[mainClass,int(p)]=tokens_without_sw\n",
    "\n",
    "            # append into train and test set\n",
    "            if(count <= train_count[mainClass]):\n",
    "                Train_documents.append((mainClass,int(p))) #\n",
    "                Train_set.append(tokens_without_sw)\n",
    "                \n",
    "#                 Train_label.append(mainClass)\n",
    "            else:\n",
    "                Test_documents.append((mainClass,int(p)))\n",
    "#                 Test_label.append(mainClass)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"train documents\",len(Train_documents))\n",
    "    print(\"test documents\",len(Test_documents))\n",
    "    print(\"total\", len(Train_documents)+len(Test_documents))\n",
    "    \n",
    "    return Train_documents, Train_set ,Test_documents ,alldocDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tfidf score\n",
    "\n",
    "def tfidfCalculation(term_index, alldocDict,Train_documents):\n",
    "    f = open(\"TFIDFdict_allDocs.txt\",\"w\")\n",
    "    \n",
    "    idf={}\n",
    "    for word in term_index.keys():\n",
    "        df=0\n",
    "        for doc in alldocDict.keys():\n",
    "            if word in alldocDict[doc]:\n",
    "                df+=1\n",
    "        idf[word]=math.log(len(Train_documents)/(df))\n",
    "        \n",
    "\n",
    "    for word in term_index.keys():\n",
    "        for doc in alldocDict.keys():      \n",
    "            if word in alldocDict[doc]:\n",
    "    #             print('word found in ',doc)   \n",
    "                term_index[word][doc]=((alldocDict[doc].count(word))*idf[word])  #calculate tfidf\n",
    "            else:\n",
    "                term_index[word][doc]=0\n",
    "                \n",
    "    f.write(str(term_index))\n",
    "    f.close()\n",
    "                \n",
    "    return term_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_documents, Test_documents, tfidf_dict\n",
    "\n",
    "def formVectors(Train_documents, Test_documents, tfidf_dict):\n",
    "\n",
    "    #form train and test vectors\n",
    "    trainDoc_vect={}\n",
    "    testDoc_vect={}\n",
    "    # for word in tfidf_dict.keys():\n",
    "\n",
    "    f1 = open(\"TrainDocs_vector.txt\",\"w\")\n",
    "    f2 = open(\"TestDocs_allDocs.txt\",\"w\")\n",
    "\n",
    "    for docid in Train_documents:\n",
    "        trainDoc_vect[docid]=[]\n",
    "        for word in tfidf_dict.keys():\n",
    "            trainDoc_vect[docid].append(tfidf_dict[word][docid])\n",
    "\n",
    "    for docid in Test_documents:\n",
    "        testDoc_vect[docid]=[]\n",
    "        for word in tfidf_dict.keys():\n",
    "            testDoc_vect[docid].append(tfidf_dict[word][docid])\n",
    "\n",
    "    f1.write(str(trainDoc_vect))\n",
    "    f1.close()\n",
    "    f2.write(str(testDoc_vect))\n",
    "    f2.close()\n",
    "    \n",
    "    return trainDoc_vect,testDoc_vect\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train word index\n",
    "\n",
    "def createIndex(Train_documents):\n",
    "    word_index={}\n",
    "    for doc in range(len(Train_documents)):\n",
    "        for word in Train_documents[doc]:\n",
    "            if word not in word_index:\n",
    "                word_index[word]={}\n",
    " \n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of train data in each class [71, 87, 186, 103, 70]\n"
     ]
    }
   ],
   "source": [
    "list,classes=getdatafromfile()\n",
    "# noOfdata=filesInEachClass(list)\n",
    "train_count=trainCount(list,0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train documents 517\n",
      "test documents 220\n",
      "total 737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Train_documents, Train_set, Test_documents, alldocDict=preprocessData(list,classes,train_count)\n",
    "# print(\"train Ids:\\n\",Train_documents)\n",
    "# print(\"test Ids:\\n\", Test_documents)\n",
    "\n",
    "# print(alldocDict[0,1])       # [0-->classID,1-->docID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=createIndex(Train_set) # index of words in training set(train_set has all the words that are in training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict=tfidfCalculation(word_index, alldocDict,Train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVector,testVector=formVectors(Train_documents, Test_documents, tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(testVector)+len(trainVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 72)\n",
      "(0, 73)\n",
      "(0, 74)\n",
      "(0, 75)\n",
      "(0, 76)\n",
      "(0, 77)\n",
      "(0, 78)\n",
      "(0, 79)\n",
      "(0, 80)\n",
      "(0, 81)\n",
      "(0, 82)\n",
      "(0, 83)\n",
      "(0, 84)\n",
      "(0, 85)\n",
      "(0, 86)\n",
      "(0, 87)\n",
      "(0, 88)\n",
      "(0, 89)\n",
      "(0, 90)\n",
      "(0, 91)\n",
      "(0, 92)\n",
      "(0, 93)\n",
      "(0, 94)\n",
      "(0, 95)\n",
      "(0, 96)\n",
      "(0, 97)\n",
      "(0, 98)\n",
      "(0, 99)\n",
      "(0, 100)\n",
      "(0, 101)\n",
      "(1, 88)\n",
      "(1, 89)\n",
      "(1, 90)\n",
      "(1, 91)\n",
      "(1, 92)\n",
      "(1, 93)\n",
      "(1, 94)\n",
      "(1, 95)\n",
      "(1, 96)\n",
      "(1, 97)\n",
      "(1, 98)\n",
      "(1, 99)\n",
      "(1, 100)\n",
      "(1, 101)\n",
      "(1, 102)\n",
      "(1, 103)\n",
      "(1, 104)\n",
      "(1, 105)\n",
      "(1, 106)\n",
      "(1, 107)\n",
      "(1, 108)\n",
      "(1, 109)\n",
      "(1, 110)\n",
      "(1, 111)\n",
      "(1, 112)\n",
      "(1, 113)\n",
      "(1, 114)\n",
      "(1, 115)\n",
      "(1, 116)\n",
      "(1, 117)\n",
      "(1, 118)\n",
      "(1, 119)\n",
      "(1, 120)\n",
      "(1, 121)\n",
      "(1, 122)\n",
      "(1, 123)\n",
      "(1, 124)\n",
      "(2, 187)\n",
      "(2, 188)\n",
      "(2, 189)\n",
      "(2, 190)\n",
      "(2, 191)\n",
      "(2, 192)\n",
      "(2, 193)\n",
      "(2, 194)\n",
      "(2, 195)\n",
      "(2, 196)\n",
      "(2, 197)\n",
      "(2, 198)\n",
      "(2, 199)\n",
      "(2, 200)\n",
      "(2, 201)\n",
      "(2, 202)\n",
      "(2, 203)\n",
      "(2, 204)\n",
      "(2, 205)\n",
      "(2, 206)\n",
      "(2, 207)\n",
      "(2, 208)\n",
      "(2, 209)\n",
      "(2, 210)\n",
      "(2, 211)\n",
      "(2, 212)\n",
      "(2, 213)\n",
      "(2, 214)\n",
      "(2, 215)\n",
      "(2, 216)\n",
      "(2, 217)\n",
      "(2, 218)\n",
      "(2, 219)\n",
      "(2, 220)\n",
      "(2, 221)\n",
      "(2, 222)\n",
      "(2, 223)\n",
      "(2, 224)\n",
      "(2, 225)\n",
      "(2, 226)\n",
      "(2, 227)\n",
      "(2, 228)\n",
      "(2, 229)\n",
      "(2, 230)\n",
      "(2, 231)\n",
      "(2, 232)\n",
      "(2, 233)\n",
      "(2, 234)\n",
      "(2, 235)\n",
      "(2, 236)\n",
      "(2, 237)\n",
      "(2, 238)\n",
      "(2, 239)\n",
      "(2, 240)\n",
      "(2, 241)\n",
      "(2, 242)\n",
      "(2, 243)\n",
      "(2, 244)\n",
      "(2, 245)\n",
      "(2, 246)\n",
      "(2, 247)\n",
      "(2, 248)\n",
      "(2, 249)\n",
      "(2, 250)\n",
      "(2, 251)\n",
      "(2, 252)\n",
      "(2, 253)\n",
      "(2, 254)\n",
      "(2, 255)\n",
      "(2, 256)\n",
      "(2, 257)\n",
      "(2, 258)\n",
      "(2, 259)\n",
      "(2, 260)\n",
      "(2, 261)\n",
      "(2, 262)\n",
      "(2, 263)\n",
      "(2, 264)\n",
      "(2, 265)\n",
      "(3, 104)\n",
      "(3, 105)\n",
      "(3, 106)\n",
      "(3, 107)\n",
      "(3, 108)\n",
      "(3, 109)\n",
      "(3, 110)\n",
      "(3, 111)\n",
      "(3, 112)\n",
      "(3, 113)\n",
      "(3, 114)\n",
      "(3, 115)\n",
      "(3, 116)\n",
      "(3, 117)\n",
      "(3, 118)\n",
      "(3, 119)\n",
      "(3, 120)\n",
      "(3, 121)\n",
      "(3, 122)\n",
      "(3, 123)\n",
      "(3, 124)\n",
      "(3, 125)\n",
      "(3, 126)\n",
      "(3, 127)\n",
      "(3, 128)\n",
      "(3, 129)\n",
      "(3, 130)\n",
      "(3, 131)\n",
      "(3, 132)\n",
      "(3, 133)\n",
      "(3, 134)\n",
      "(3, 135)\n",
      "(3, 136)\n",
      "(3, 137)\n",
      "(3, 138)\n",
      "(3, 139)\n",
      "(3, 140)\n",
      "(3, 141)\n",
      "(3, 142)\n",
      "(3, 143)\n",
      "(3, 144)\n",
      "(3, 145)\n",
      "(3, 146)\n",
      "(3, 147)\n",
      "(4, 71)\n",
      "(4, 72)\n",
      "(4, 73)\n",
      "(4, 74)\n",
      "(4, 75)\n",
      "(4, 76)\n",
      "(4, 77)\n",
      "(4, 78)\n",
      "(4, 79)\n",
      "(4, 80)\n",
      "(4, 81)\n",
      "(4, 82)\n",
      "(4, 83)\n",
      "(4, 84)\n",
      "(4, 85)\n",
      "(4, 86)\n",
      "(4, 87)\n",
      "(4, 88)\n",
      "(4, 89)\n",
      "(4, 90)\n",
      "(4, 91)\n",
      "(4, 92)\n",
      "(4, 93)\n",
      "(4, 94)\n",
      "(4, 95)\n",
      "(4, 96)\n",
      "(4, 97)\n",
      "(4, 98)\n",
      "(4, 99)\n",
      "(4, 100)\n"
     ]
    }
   ],
   "source": [
    "testsimilarity={}\n",
    "for testid in testVector:\n",
    "    testsimilarity[testid]={}\n",
    "    for trainid in trainVector:\n",
    "        testsimilarity[testid][trainid]=distance.euclidean(testVector[testid], trainVector[trainid])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3NN of doc (4,100)-->  [(4, 51), (4, 47), (2, 4)]\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 2\n",
      "0 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 2\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "3 0\n",
      "3 2\n",
      "3 2\n",
      "3 0\n",
      "3 2\n",
      "3 0\n",
      "3 0\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "3 0\n",
      "3 0\n",
      "3 2\n",
      "3 0\n",
      "3 2\n",
      "3 0\n",
      "3 3\n",
      "3 3\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "3 0\n",
      "3 3\n",
      "3 2\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 0\n",
      "3 2\n",
      "3 3\n",
      "3 2\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "4 4\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 4\n",
      "4 4\n",
      "4 0\n",
      "4 4\n",
      "4 0\n",
      "4 4\n",
      "4 2\n",
      "4 4\n",
      "4 4\n",
      "4 2\n",
      "4 2\n",
      "4 0\n",
      "4 4\n",
      "4 2\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 2\n",
      "4 4\n",
      "4 2\n",
      "4 4\n",
      "4 4\n",
      "4 2\n",
      "4 4\n",
      "4 2\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "# print(\"3NN of doc (4,100)--> \",sorted(testsimilarity[(4,100)], key=testsimilarity[(4,100)].get)[0:3])\n",
    "# # n1,n2,n3=sorted(testsimilarity[(4,100)], key=testsimilarity[(4,100)].get)[0:3]\n",
    "# # newList=[n1[0],n2[0],n3[0]]\n",
    "# # frequentNN=max(set(newList), key = newList.count)\n",
    "# # print(newList,frequentNN)\n",
    "\n",
    "correct=0\n",
    "no=0\n",
    "for i in testsimilarity:\n",
    "    n1,n2,n3=sorted(testsimilarity[i], key=testsimilarity[i].get)[0:3]\n",
    "    newList=[n1[0],n2[0],n3[0]]\n",
    "    frequentNN=max(set(newList), key = newList.count)\n",
    "    print(i[0],frequentNN)\n",
    "    if(i[0]==frequentNN):\n",
    "        correct+=1\n",
    "    else:\n",
    "        no+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "0.6318181818181818\n"
     ]
    }
   ],
   "source": [
    "print(no)\n",
    "print(correct/(no+correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
