{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "from array import array\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.spatial import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "import random\n",
    "import tkinter as tk\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tk.Tk() \n",
    "\n",
    "r.title('Counting Seconds') \n",
    "\n",
    "button = tk.Button(r, text='Stop', width=25, command=r.destroy) \n",
    "\n",
    "button.grid() \n",
    "\n",
    "r.mainloop() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdatafromfile():\n",
    "    classes=['athletics','cricket','football','rugby','tennis']\n",
    "    list=[glob.glob('bbcsport/{}/*'.format(classes[i])) for i in range(len(classes))]\n",
    "    return list,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeanClusteringClass:\n",
    "    \n",
    "    def __init__(self,list,classes):\n",
    "        self.list=list\n",
    "        self.classes=classes\n",
    "        self.totalfiles=[len(i) for i in list]\n",
    "        print(\"Total files in each class: \",self.totalfiles)      \n",
    "        \n",
    "    def preprocessData(self):\n",
    "\n",
    "        self.alldocDict={}\n",
    "        Init_corpus=[]\n",
    "        \n",
    "        for mainClass in range(len(self.list)): \n",
    "            for subFiles in range(len(self.list[mainClass])):\n",
    "                f=open(self.list[mainClass][subFiles],'r')\n",
    "                tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                # convert to lower case\n",
    "                fullfile = tokenizer.tokenize(f.read().lower())\n",
    "                # stem document\n",
    "                stemmedDocs=[porter_stemmer.stem(word) for word in fullfile]\n",
    "                #trimming the file name and removing redundant '.txt'\n",
    "                p=os.path.basename(self.list[mainClass][subFiles])\n",
    "                p=p.split('.')[0]\n",
    "                # remove all tokens that are not alphabetic and stop words\n",
    "                tokens_without_sw = [word for word in stemmedDocs if word not in stop_words and word.isalpha()]\n",
    "                self.alldocDict[mainClass,int(p)]=tokens_without_sw\n",
    "                Init_corpus.append(tokens_without_sw)\n",
    "                \n",
    "        self.UniqueWord_corpus=set().union(*Init_corpus)\n",
    " \n",
    "    \n",
    "    def getAllDocs(self):\n",
    "        return self.alldocDict\n",
    "    \n",
    "    def featureExtractionDf(self):\n",
    "        print(len(self.UniqueWord_corpus))     \n",
    "        self.corpus=[]\n",
    "        self.idf={}\n",
    "        for word in self.UniqueWord_corpus:\n",
    "            df=0\n",
    "            for v in self.alldocDict.values():\n",
    "                if word in v:\n",
    "                    df+=1\n",
    "            if(df>2):\n",
    "                self.corpus.append(word)\n",
    "                self.idf[word]=round(math.log(737/df),5)\n",
    "                \n",
    "        print(len(self.corpus))\n",
    "#         print(self.idf.keys())\n",
    "        \n",
    "    def calculatetfidfAndFormVectors(self):\n",
    "              \n",
    "        tfidf={}\n",
    "        for word in self.idf.keys():\n",
    "            tfidf[word]={}\n",
    "            for doc in self.alldocDict.keys():\n",
    "                if word in self.alldocDict[doc]:\n",
    "                    tfidf[word][doc]=(self.alldocDict[doc].count(word)*self.idf[word])\n",
    "                else:\n",
    "                    tfidf[word][doc]=0\n",
    "                    \n",
    "        f = open(\"TFIDF_Clustering_allDocs.txt\",\"w\")          \n",
    "        f.write(str(tfidf))\n",
    "        f.close()\n",
    "        print(\"tfidf calculated\")\n",
    "        \n",
    "        self.docVect={}\n",
    "        for docid in self.alldocDict.keys():\n",
    "            \n",
    "            self.docVect[docid]=[tfidf[word][docid] for word in self.corpus]\n",
    "\n",
    "        f = open(\"DocVectors_Clustering.txt\",\"w\")          \n",
    "        f.write(str(self.docVect))\n",
    "        f.close()\n",
    "        \n",
    "    \n",
    "    def getInitialCentroid(self,k=5):\n",
    "\n",
    "        centro=[]\n",
    "        cent=random.sample(self.docVect.keys(),k)\n",
    "        print(\"initial seed-->\",cent)\n",
    "\n",
    "        for i in range(self.k):\n",
    "#             print(\"centroid vector-->\",cent[i],\"-->\",self.docVect[cent[i]])\n",
    "            centro.append(self.docVect[cent[i]])\n",
    "\n",
    "        \n",
    "        return centro\n",
    "    \n",
    "    def purity(self,cluster):\n",
    "        finalLabels=[[],[],[],[],[]]\n",
    "        fc=[]\n",
    "        p=0\n",
    "        for i in range(len(cluster)):\n",
    "            print(\"----------------\",i,\"----------------\")\n",
    "            for j in cluster[i]:\n",
    "                finalLabels[i].append(j[0])\n",
    "            \n",
    "            print(finalLabels[i])\n",
    "            frequentLabel=max(set(finalLabels[i]), key = finalLabels[i].count)\n",
    "            if frequentLabel not in finalLabels:\n",
    "            cnt=finalLabels[i].count(frequentLabel)\n",
    "            fc.append(cnt) \n",
    "            print(\"frequent class=\",frequentLabel,\",count=\",cnt)\n",
    "            \n",
    "        s=sum(fc)\n",
    "        print(\"sum=\",s)\n",
    "        p=s/737\n",
    "        print(\"purity\",p)\n",
    "            \n",
    "        \n",
    "        return p\n",
    "        \n",
    "                \n",
    "    def formCluster(self,cent,cluster):\n",
    "        cluster=[[],[],[],[],[]]\n",
    "        \n",
    "        #traversing through all docs\n",
    "        for docId in self.docVect.keys():\n",
    "            dist=[]     \n",
    "            #doc distance with all centroids\n",
    "            for centdocVect in cent:\n",
    "                dist.append(distance.cosine(self.docVect[docId], centdocVect))\n",
    "            \n",
    "            cluster[dist.index(min(dist))].append(docId) \n",
    "            \n",
    "        return cluster\n",
    "        \n",
    "    def KmeanCentroid(self,cluster):\n",
    "        \n",
    "        centroid=[]\n",
    "        \n",
    "        for c in range(len(cluster)):\n",
    "            \n",
    "            lv=[] #temporary list vector of each cluster\n",
    "            lv=[self.docVect[cid] for cid in cluster[c]]\n",
    "            s=np.array(lv)\n",
    "            centroidMean=np.mean(s,axis=0)\n",
    "            centroidMean=centroidMean.tolist()\n",
    "            centroid.append(centroidMean)        \n",
    "\n",
    "        return centroid\n",
    "        \n",
    "        \n",
    "    def KmeanClustering(self,k):\n",
    "        self.k=k\n",
    "        centroid=self.getInitialCentroid(k) #it has id of cendroid docs\n",
    "        cluster=[[]]*self.k #initialing empty list of lists\n",
    "        \n",
    "        i=0\n",
    "        while(i<30): \n",
    "            print(\"Iteration\",i+1)\n",
    "            i+=1\n",
    "            cluster=self.formCluster(centroid,cluster)\n",
    "            newCentroid=[]\n",
    "            newCentroid=self.KmeanCentroid(cluster)\n",
    "            \n",
    "#             print(newCentroid)\n",
    "    \n",
    "            if(sorted(centroid) == sorted(newCentroid)):\n",
    "                print(\"same centroids found!\")\n",
    "                p=self.purity(cluster)\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"centroids not same!\")\n",
    "                centroid=[]\n",
    "                centroid=newCentroid\n",
    "                       \n",
    "        return\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "list,classes=getdatafromfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in each class:  [101, 124, 265, 147, 100]\n",
      "9130\n",
      "4336\n"
     ]
    }
   ],
   "source": [
    "clf=KMeanClusteringClass(list,classes)\n",
    "clf.preprocessData()\n",
    "clf.featureExtractionDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf calculated\n"
     ]
    }
   ],
   "source": [
    "clf.calculatetfidfAndFormVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InitialCentroid=clf.getInitialCentroid()\n",
    "# print(InitialCentroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial seed--> [(1, 25), (2, 58), (4, 27), (2, 257), (4, 81)]\n",
      "Iteration 1\n",
      "centroids not same!\n",
      "Iteration 2\n",
      "centroids not same!\n",
      "Iteration 3\n",
      "centroids not same!\n",
      "Iteration 4\n",
      "centroids not same!\n",
      "Iteration 5\n",
      "centroids not same!\n",
      "Iteration 6\n",
      "centroids not same!\n",
      "Iteration 7\n",
      "centroids not same!\n",
      "Iteration 8\n",
      "centroids not same!\n",
      "Iteration 9\n",
      "centroids not same!\n",
      "Iteration 10\n",
      "centroids not same!\n",
      "Iteration 11\n",
      "centroids not same!\n",
      "Iteration 12\n",
      "centroids not same!\n",
      "Iteration 13\n",
      "centroids not same!\n",
      "Iteration 14\n",
      "centroids not same!\n",
      "Iteration 15\n",
      "centroids not same!\n",
      "Iteration 16\n",
      "centroids not same!\n",
      "Iteration 17\n",
      "same centroids found!\n",
      "---------------- 0 ----------------\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "frequent class= 1 ,count= 118\n",
      "---------------- 1 ----------------\n",
      "[0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3]\n",
      "frequent class= 2 ,count= 235\n",
      "---------------- 2 ----------------\n",
      "[1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "frequent class= 3 ,count= 141\n",
      "---------------- 3 ----------------\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4]\n",
      "frequent class= 0 ,count= 76\n",
      "---------------- 4 ----------------\n",
      "[2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "frequent class= 4 ,count= 96\n",
      "sum= 666\n",
      "purity 0.903663500678426\n"
     ]
    }
   ],
   "source": [
    "clf.KmeanClustering(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
